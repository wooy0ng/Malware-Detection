import os
import re
import sys
import time
import torch
import pefile
import subprocess
import multiprocessing
import numpy as np
import pandas as pd
import pickle as pkl
from tqdm import tqdm
from pathlib import Path
from base import BaseDataLoader
from torch.utils.data import Dataset


from sklearn.feature_extraction.text import TfidfVectorizer


from typing import Optional, Union, List, Tuple, TypedDict
from capstone import Cs, CS_ARCH_X86, CS_MODE_32, CS_MODE_64


class OpcodeDataset(Dataset):
    def __init__(self, 
            metadata_path: Optional[Union[str, os.PathLike]], 
            data_dir: Optional[Union[str, os.PathLike]],
            num_workers:int,
            logger,
            save_dir,
            n_gram,
            training
        ):
        self.logger = logger
        self.num_workers = num_workers
        self.n_gram = n_gram
        self.training = training
        
        self._metadata_path = Path(metadata_path)
        self._data_dir = Path(data_dir)
        assert self._metadata_path.exists(), "cannot find metadata.csv, this file must be exists."
        assert self._data_dir.exists(), "cannot find embed_passage directory, this file must be exists."
        
        self._save_dir = Path(save_dir)
        self._save_dir.mkdir(parents=True, exist_ok=True)
        
        self._metadata = pd.read_csv(self._metadata_path)
        
        self._passage = None
        self._get_opcode_sequences(self._data_dir, self._metadata, self.num_workers)
        
        self._embed = None
        self._embed_passage = None
        self._passage_embedding(self.passage)
    
    def _get_opcode_sequences(self, data_dir:Path, metadata, num_workers:int) -> List[Tuple]:
        mode = 'train' if self.training else 'test'
        
        seq_path = str(self._save_dir / f'{mode}_not_embed.bin')
        if os.path.exists(seq_path):
            with open(seq_path, 'rb') as f:
                message = "passage already exists. load passage from {}".format(seq_path)
                self.logger.info(message)
                self._passage = pkl.load(f)
        else:
            self._passage = []
            with multiprocessing.Pool(processes=num_workers) as pool:
                for _, row in tqdm(metadata.iterrows(), desc='pefile to opcode...', total=len(metadata)):
                    fname, label = row['file_name'], row['label']
                    fpath = data_dir / fname
                    if not os.path.exists(fpath):
                        message = "Warning: {} not in {}, ignored.".format(fname, str(data_dir))
                        self.logger.warning(message)
                    else:
                        result = pool.apply_async(self._process_file, args=(fpath, label))
                        self._passage.append(result.get())    # wait for the process to complete 
                                
            with open(seq_path, 'wb') as f:
                message = "save not_embed passage, save path is {}".format(seq_path)
                self.logger.info(message)
                pkl.dump(self._passage, f)
    
    def _process_file(self, fpath, label):
        pe = pefile.PE(fpath)
        opcode_sequence = self._get_opcode_sequence(pe)
        return (fpath.name, label, opcode_sequence)

    def _get_opcode_sequence(self, pe:pefile.PE):
        ext = []
        for section in pe.sections:
            section_name = section.Name.decode('utf-8', errors='ignore').strip('\x00')
            if section_name == '.text':     # code section
                code = section.get_data()

                # set disassembler architecture
                if pe.FILE_HEADER.Machine == 0x14C:  # 32bit
                    md = Cs(CS_ARCH_X86, CS_MODE_32)
                elif pe.FILE_HEADER.Machine == 0x8664:  # 64bit
                    md = Cs(CS_ARCH_X86, CS_MODE_64)
                else:
                    print("Unsupported architecture")
                    sys.exit(-1)

                # append opcode sequence
                for op in md.disasm(code, section.VirtualAddress):
                    ext.append(op.mnemonic)

        context = ' '.join(ext)
        return context
    
    def _passage_embedding(self, passage):
        """ passage embedding using bm25 """
        embed_path = str(self._save_dir / 'embed_func.bin')
        if os.path.exists(embed_path):
            with open(embed_path, 'rb') as f:
                message = "embed already exists. load embed from {}".format(embed_path)
                self.logger.info(message)
                self._embed = pkl.load(f)
        else:
            # ======================================== #
            # encode with TFIDF using spacy tokenizer  #
            # ======================================== #
            psg = list(map(lambda x: x[-1], passage))
            
            # self._embed = BM25(ngram_range=(self.n_gram, self.n_gram))
            self._embed = TfidfVectorizer(ngram_range=(self.n_gram, self.n_gram))
            self._embed.fit(psg)
            with open(embed_path, 'wb') as f:
                message = 'save embed object, save path is {}'.format(embed_path)
                self.logger.info(message)
                pkl.dump(self._embed, f)
        
        embed_passage_path = str(self._save_dir / 'embed_passage.bin')
        if os.path.exists(embed_passage_path) and self.training:
            with open(embed_passage_path, 'rb') as f:
                message = "embed passage already exists. load embed from {}".format(embed_passage_path)
                self.logger.info(message)
                self._embed_passage = pkl.load(f)
        else:
            file_names = list(map(lambda x: x[0], passage))
            embed_passage = self._embed.transform(list(map(lambda x: x[-1], passage)))
            labels = list(map(lambda x: x[1], passage))
            self._embed_passage = {
                "file_name": file_names,
                "embed_passage": embed_passage,
                "label": labels
            }
            
            with open(embed_passage_path, 'wb') as f:
                message = 'save embed passage object, save path is {}'.format(embed_path)
                self.logger.info(message)
                pkl.dump(self._embed_passage, f)

    @property
    def passage(self):
        return self._passage
    
    @property
    def embed(self):
        return self._embed
    
    @property
    def embed_passage(self):
        return self._embed_passage
    
    @property
    def metadata(self):
        return self._metadata

    def __len__(self):
        return len(self.passage)
    
    def __getitem__(self, idx):
        # using for dataloader
        embed_passage = self.embed_passage['embed_passage']
        return {
            'file_name': self.embed_passage['file_name'][idx],
            'embed_passage': torch.tensor(embed_passage[idx].toarray()).to(dtype=torch.float).squeeze(),
            'label': torch.tensor(self.embed_passage['label'][idx])
        }
    
class OpcodeDataLoader(BaseDataLoader):
    """ MalwareDetection data loader """
    def __init__(self, 
            data_dir, 
            metadata_path, 
            logger, 
            batch_size=1, 
            validation_split:Union[float, int]=0.0,
            num_workers:int=1,
            n_gram:int=1,
            save_dir: Union[str, os.PathLike]='saved/passage',  # object save dir
            shuffle=True, 
            training=True
        ):
        self.embed_passage = OpcodeDataset(
            data_dir=data_dir,
            metadata_path=metadata_path,
            num_workers=num_workers,
            n_gram=n_gram,
            logger=logger,
            save_dir=save_dir,
            training=training
        )
        super().__init__(self.embed_passage, batch_size, shuffle, validation_split, num_workers)
    

class SpecDataLoader(BaseDataLoader):
    def __init__(self, 
            data_dir, 
            metadata_path, 
            logger, 
            batch_size=1, 
            validation_split:Union[float, int]=0.0,
            num_workers:int=1,
            save_dir: Union[str, os.PathLike]='saved/passage',  # object save dir
            shuffle=True, 
            training=True
        ):
        self.dataset = SpecDataset(
            data_dir=data_dir,
            metadata_path=metadata_path,
            save_dir=save_dir,
            logger=logger,
        )
        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)

class SpecDataset(Dataset):
    def __init__(self,
            data_dir,
            metadata_path,
            save_dir,
            logger,
        ):
        self.data_dir = Path(data_dir)
        self.metadata_path = Path(metadata_path)
        
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logger
        
        self.data_directory_list = [
            'DIRECTORY_ENTRY_DEBUG', 
            'DIRECTORY_ENTRY_EXPORT', 
            'DIRECTORY_ENTRY_LOAD_CONFIG',
            'DIRECTORY_ENTRY_RESOURCE', 
            'DIRECTORY_ENTRY_BASERELOC', 
            'DIRECTORY_ENTRY_TLS'
        ]
        
        self.normal_section_names = ['.text', '.rdata', '.data', '.pdata', '.rsrc', '.idata', '.bss', '.code', '.edata']
        
        # feature_extractor
        self.embed_passage = None
        self._feature_extractor(self.data_dir, self.save_dir)
    
    def _load_features(self, metadata_path, feature_path):
        df = pd.read_csv(metadata_path)
        rows = open(feature_path, 'r').readlines()
        
        fname, embed, label = [], [], []
        for row in tqdm(rows, total=len(rows)):
            final_features = []
            features = row.strip('\n').split(',')
            
            try:
                tmp = df.loc[df["file_name"] == features[-1]]
                fname.append(tmp["file_name"].values[0])
                label.append(float(tmp['label']))
            
                for feature in features[0:-1]:    
                    final_features.append(float(feature))
                embed.append(final_features)
            except:
                pass    # not in embed_passage
        
        self.embed_passage = embed
        return {
            'file_name': fname,
            'embed_passage': embed,     # embed vectors
            'label': label
        }
    
    def _feature_extractor(self, data_dir, save_dir):
        """ 
        author: @persShin
        """
        start_time = time.time()
        samples = os.listdir(data_dir)
        
        tmp_sample = []
        tmp_tmp = []
        tmp_count = []
        tmp_checksum = []
        tmp_entro = []
        tmp_empty_section_names = []
        tmp_section_name_features0 = []
        tmp_section_name_features1 = []
        
        self.logger.info("start feature extraction...")
        save_path = save_dir / 'spec_features.txt'
        if not os.path.exists(save_path):
            features_outputfile = open(save_path, 'w')
            
            for n, sample in enumerate(tqdm(samples, total=len(samples))):
                # self.logger.info("{} {}".format(n, sample))
                pe = pefile.PE(data_dir / sample)
                tmp_sample.append(sample)
                
                # ----------------- Data Directories --------------------
                if pe.OPTIONAL_HEADER.DllCharacteristics >= 1:
                    features_outputfile.write('1,')
                else:
                    features_outputfile.write('0,')
                
                tmp = '0'
                for data_directory in self.data_directory_list:
                    tmp += str(self.data_directory_checker(pe, data_directory))
                tmp_tmp.append(tmp)
                features_outputfile.write('{},'.format(int(tmp, 2)))
                
                # ---------------------- file_info -----------------------
                count = 0
                try:
                    for entry in pe.FileInfo:
                        if entry[0].Key == b'StringFileInfo':
                            entry = entry[0]
                            for st in entry.StringTable:
                                for entry in (st.entries.items()):
                                    count += 1
                        if entry[1].Key == b'StringFileInfo':
                            entry = entry[1]
                            for st in entry.StringTable:
                                for entry in (st.entries.items()):
                                    count += 1
                    features_outputfile.write('{},'.format(count))
                    tmp_count.append(count)
                except:
                    features_outputfile.write('{},'.format(count))
                    tmp_count.append(count)
                
                # ---------------------- checksum ------------------------
                try:
                    checksum = pe.OPTIONAL_HEADER.CheckSum
                    features_outputfile.write('0,'.format(sample)) if checksum == 0 else features_outputfile.write(
                        '1,'.format(sample))
                    if checksum > 0:
                        checksum = 1
                    tmp_checksum.append(checksum)
                except:
                    features_outputfile.write('0,'.format(sample))
                    tmp_checksum.append(0)            
                
                # ------------------------- entropy ---------------------------
                entropies = self.entropy(sample, data_dir)
                for n,entro in enumerate(entropies):
                    if n>2:
                        print(sample)
                    features_outputfile.write('{},'.format(entro))
                    tmp_entro.append(entro)
                
                # ----------------------- section names -----------------------
                section_names = []
                try:
                    sections = pe.sections
                    for section in sections:
                        name = (section.Name).decode('utf-8')
                        name = name.replace('\x00', '')
                        section_names.append(name)
                except:
                    pass
                    
                section_name_features = self.section_name_checker(section_names)
                features_outputfile.write('{},{},'.format(section_name_features[0], section_name_features[1]))
                tmp_section_name_features0.append(section_name_features[0])
                tmp_section_name_features1.append(section_name_features[1])
                
                empty_section_names = self.empty_section_name_checker(section_names)
                tmp_empty_section_names.append(empty_section_names)
                features_outputfile.write('{},{}\n'.format(empty_section_names, sample))
        
        end_time = time.time()
        print('feature extraction time: {}s'.format(end_time - start_time))
        self.logger.info("complete")
        
        self.embed_passage = self._load_features(
            metadata_path=self.metadata_path,
            feature_path=save_path
        )
        return
    
    def entropy(self, name, path):
        entropy_list = []
    
        #print('qwe')
        entropy = subprocess.check_output("ent '{}' | head -n 1 | cut -d' ' -f 3".format((path / name)),
                                        shell=True).decode('utf8')
        
        entropy_list.append(entropy[0:-1])
        pe = pefile.PE(path / name)
        text_flag = False
        data_flag = False
        for section in pe.sections:
            try:
                section_name = (section.Name).decode('utf-8')
                section_name = section_name.replace('\x00','')
                if section_name =='.text':
                    text_entropy = section.get_entropy()
                    text_flag = True
                elif section_name =='.data':
                    data_entropy = section.get_entropy()
                    data_flag = True
            except:
                continue
        entropy_list.append(text_entropy if text_flag else -1)
        entropy_list.append(data_entropy if data_flag else -1)
        return entropy_list
    
    @staticmethod
    def data_directory_checker(pe, data_directory_name) -> int:
        try:
            if getattr(pe, data_directory_name):
                return 1
            else:
                return 0
        except:
            return 0
    
    def section_name_checker(self, section_names):
        """
        :param section_names:
        an array of section names of a program
        :return:
        a 1*2d array that indicate number of nonsuspicious sections and number of suspicious sections,respectively
        """
        number_of_suspicious_names = 0
        number_of_nonsuspicious_names = 0
        for name in section_names:
            if name in self.normal_section_names:
                number_of_nonsuspicious_names += 1
            else:
                number_of_suspicious_names += 1

        return number_of_suspicious_names,number_of_nonsuspicious_names
    
    def empty_section_name_checker(self, section_names) -> int:
        #---- normalize names --------
        for i in range(len(section_names)):
            section_names[i] = re.sub(' +', ' ',section_names[i])

        if '' in section_names or ' ' in section_names:
            # print(file_name)
            return 0
        else:
            return 1
        
    def __len__(self):
        return len(self.embed_passage['embed_passage'])
    
    def __getitem__(self, idx):
        embed = self.embed_passage['embed_passage'][idx]
        label = self.embed_passage['label'][idx]
        return {
            'file_name': self.embed_passage['file_name'][idx],
            'embed_passage': torch.tensor(embed).to(dtype=torch.float).squeeze(),
            'label': torch.tensor(label)
        }
        
        
        
class APIDataLoader(BaseDataLoader):
    def __init__(self,
            apidata_dir,
            data_dir,
            metadata_path,
            logger,
            batch_size=1, 
            validation_split:Union[float, int]=0.0,
            num_workers:int=1,
            save_dir: Union[str, os.PathLike]='saved/passage',  # object save dir
            shuffle=True, 
            training=True
        ):
        self.dataset = APIDataset(
            apidata_dir=apidata_dir,
            data_dir=data_dir,
            metadata_path=metadata_path,
            save_dir=save_dir,
            logger=logger,
        )
        
        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)   


class APIDataset(Dataset):
    def __init__(self,
            apidata_dir,
            data_dir,
            metadata_path,
            save_dir,
            logger,
        ):
        self.functions_list = [
            "GetUserDefaultLCID", "SizeofResource", "MoveFileW", "LockResource", "LocalAlloc",
                "InitializeCriticalSection", "GetThreadLocale", "GetModuleHandleA", "GetLocaleInfoA", 
                "ExitThread", "EnumCalendarInfoA", "DeleteFileW", "CharLowerBuffA", 
                "TryEnterCriticalSection", "SysReAllocStringLen", "SysFreeString", "SysAllocStringLen", 
                "SuspendThread", "SetThreadLocale", "SetFilePointer", "ResumeThread", "ReadFile", 
                "LoadResource", "LeaveCriticalSection", "GetThreadPriority", "GetStdHandle", 
                "GetStartupInfoA", "GetProcessHeap", "GetProcAddress", "GetCurrentThreadId", 
                "FreeResource", "FreeLibrary", "FindResourceExA", "EnumResourceTypesA", 
                "EnumResourceNamesA", "EnumResourceLanguagesA", "EnterCriticalSection", 
                "DeleteCriticalSection", "CreateFileW", "CreateEventA", "CompareStringA", 
                "CloseHandle", "CharUpperBuffA", "CharLowerA", "SetThreadToken", 
                "ChildWindowFromPoint", "CharToOemW", "_wcslwr", "BuildCommDCBAndTimeoutsW",
                "GetTapeParameters", "BackupEventLogA", "WriteConsoleOutputCharacterA",
                "WriteConsoleOutputAttribute"
            ]
        
        # 검사할 모듈 목록
        self.modules_list = [
            "kernel32.dll", "user32.dll", "oleaut32.dll", "ole32.dll", "SHLWAPI.DLL",
            "advapi32.dll", "SHELL32.dll", "gdi32.dll", "comctl32.dll", "MSVCRT.dll",
            "WININET.dll", "mscoree.dll", "PSAPI.DLL", "version.dll", "WS2_32.dll",
            "gdiplus.dll", "USERENV.dll", "MSVBVM60.DLL", "wsock32.dll", "WINSPOOL.DRV",
            "WTSAPI32.dll", "MSIMG32.dll", "WINMM.dll", "RPCRT4.DLL", "comdlg32.dll",
            "IPHLPAPI.DLL", "ATL.DLL", "netapi32.dll", "ntdll.dll", "MPR.DLL",
            "CRYPT32.dll", "MSVCP60.dll", "UxTheme.dll", "rtl60.bpl", "ibxpress60.bpl",
            "xmlrtl60.bpl", "Cabinet.dll", "api-ms-win-crt-utility-l1-1-0.dll", "msys-2.0.dll",
            "api-ms-win-core-console-l2-1-0.dll", "api-ms-win-core-libraryloader-l1-2-0.dll"
        ]
        
        self.data_dir = Path(data_dir)
        self.metadata_path = Path(metadata_path)
        
        
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logger

        # load functions_label
        self.apidata_dir = Path(apidata_dir)
        
        data_0 = pd.read_csv(self.apidata_dir / 'functions_label_0.csv')
        data_1 = pd.read_csv(self.apidata_dir / 'functions_label_1.csv')
        self.metadata = pd.read_csv(self.metadata_path)
        
        self.concated_data = pd.concat([data_0, data_1])
        self.concated_data.drop_duplicates(inplace=True)
        
        # train 데이터셋인지, test 데이터셋인지에 따라 다르게 데이터 처리
        self.concated_data = pd.merge(self.metadata[['file_name']], self.concated_data, how='inner')
        
        # feature_extractor
        self.embed_passage = None
        self._feature_extractor(concated_data=self.concated_data)
    
    def _feature_extractor(self, concated_data: pd.DataFrame):
        """
        author: @choisaerom
        """
        
        # 각 파일에 대해 제공된 함수 목록의 각 함수가 차지하는 비율을 계산
        grouped_data = concated_data.groupby('file_name')['function'].apply(list).reset_index()
        
        results_df = pd.DataFrame()
        results_df['file_name'] = grouped_data['file_name']
        for func in self.functions_list:
            func_percentages = []
            for _, row in grouped_data.iterrows():
                total_functions = len(row['function'])
                func_count = row['function'].count(func)
                percentage = (func_count / total_functions * 100) if total_functions > 0 else 0
                func_percentages.append(percentage)
            results_df[func] = func_percentages
        
        # 결과 DataFrame을 CSV 파일로 저장
        output_csv_path = self.apidata_dir / 'concated_functions_per.csv'
        results_df.to_csv(output_csv_path, index=False)
        ft1 = results_df.copy()
        
        # 모듈 데이터: 각 파일에 대해 모듈 비율 계산
        modules_path_0 = self.apidata_dir / 'modules_label_0.csv'
        modules_path_1 = self.apidata_dir / 'modules_label_1.csv'
        
        modules_data_0 = pd.read_csv(modules_path_0)
        modules_data_1 = pd.read_csv(modules_path_1)
        concated_modules_data = pd.concat([modules_data_0, modules_data_1])
        concated_modules_data.drop_duplicates(inplace=True)
        
        init_info = self.metadata[['file_name', 'label']]
        columns = ['file_name', 'label'] + self.modules_list
        results_df = pd.DataFrame(columns=columns)
        results_df[['file_name', 'label']] = init_info
    
        grouped_modules_data = concated_modules_data.groupby('file_name')['module'].apply(list).reset_index()
        
        for index, row in results_df.iterrows():
            file_name = row['file_name']
            if file_name in grouped_modules_data['file_name'].values:
                modules = grouped_modules_data[grouped_modules_data['file_name'] == file_name].iloc[0]['module']
                total_modules = len(modules)
                for module in self.modules_list:
                    module_count = modules.count(module)
                    percentage = (module_count / total_modules * 100) if total_modules > 0 else 0
                    results_df.at[index, module] = percentage
            else:
                results_df.loc[index, self.modules_list] = 0
    
        # 결과를 CSV 파일로 저장
        output_csv_path = self.apidata_dir / 'complete_module_percentages.csv'
        results_df.to_csv(output_csv_path, index=False)
        ft2 = results_df.copy()
        
        # 두 결과물 merge        
        self.embed_passage = pd.merge(ft1, ft2, how='right', left_on='file_name', right_on='file_name')
        self.embed_passage.dropna(axis=0, inplace=True)
        
        self.embed_passage = {
            'file_name': self.embed_passage['file_name'],
            'embed_passage': np.array(self.embed_passage.iloc[:, ~self.embed_passage.columns.isin(['file_name', 'label'])]),
            'label': self.embed_passage['label']
        }
    
    def __len__(self):
        return len(self.embed_passage['embed_passage'])
    
    def __getitem__(self, idx):
        embed = self.embed_passage['embed_passage'][idx]
        label = self.embed_passage['label'][idx]
        return {
            'file_name': self.embed_passage['file_name'][idx],
            'embed_passage': torch.tensor(embed).to(dtype=torch.float).squeeze(),
            'label': torch.tensor(label)
        }
        


        