import logging
import mlflow
import argparse
import numpy as np
import pandas as pd
import model.metric as module_metric
import data_loader.data_loaders as module_data
from pathlib import Path
from utils import read_conf
from logger import setup_logging
from collections import defaultdict


def test_logger():
    log_dir = Path('saved/log/predicted')
    log_dir.mkdir(parents=True, exist_ok=True)
    
    setup_logging(save_dir=log_dir)
    logger = logging.getLogger('predicted')
    logger.setLevel(logging.DEBUG)
    
    logger.info("evaluation mode")
    return logger

def main(config):
    # set mlflow tracking directory's path
    mlflow_dir = Path('saved/mlflow')
    tracking_url = 'file://' + str(mlflow_dir.absolute())
    mlflow.set_tracking_uri(tracking_url)
    
    logger = test_logger()   # logging
    
    data_loaders = dict()
    archs = dict()
    for dname, arch in zip(config['data_loader']['type'], config['arch']['type']):     # initialize dataloader
        obj = getattr(module_data, dname)
        kwargs = {
            'data_dir': 'data/dataset/unziped',
            'metadata_path': 'data/test_metadata.csv',
            'logger': logger,
            'training': False
        }
        if dname.startswith("API") is True:
            kwargs.update({'apidata_dir': "data/apidata"})
        
        data_loaders.update({dname: obj(**kwargs)})
        logger.info(" ")
        logger.info("        {} obj is loaded.".format(dname))
        
        loaded_model = mlflow.pyfunc.load_model(arch)
        archs.update({dname: loaded_model})

    # prediction
    all_predictions = defaultdict(list)
    all_labels = defaultdict(int)
    metrics = [getattr(module_metric, met) for met in config['metrics']]
    for dname in data_loaders.keys():
        fnames = data_loaders[dname].dataset.embed_passage['file_name']
        dataset = data_loaders[dname].dataset.embed_passage['embed_passage']
        labels = np.array(data_loaders[dname].dataset.embed_passage['label'])
        
        if dname.startswith('Opcode') is not True:
            dataset = pd.DataFrame(dataset)
            
        loaded_model = archs[dname]
        predicted = loaded_model.predict(dataset)
        
        for metric_ftn in metrics:
            score = metric_ftn(labels, predicted)
            logger.info(f"{dname} - {metric_ftn.__name__} score : {score}")
        logger.info(" ")
        
        for fname, p, label in zip(fnames, predicted.tolist(), labels.tolist()):
            all_labels[fname] = label
            all_predictions[fname].append(p)
        
    # hard voting
    labels, votes = list(), list()
    for fname in all_labels.keys():
        predictions = pd.Series(all_predictions[fname])
        labels.append(all_labels[fname])
        votes.append(predictions.value_counts().keys()[0])
        
    for metric_ftn in metrics:
        score = metric_ftn(np.array(labels), np.array(votes))
        logger.info(f"hard vote - {metric_ftn.__name__} score : {score}")
    return

if __name__ == '__main__':
    args = argparse.ArgumentParser(description='Evaluation')
    args.add_argument('-c', '--config', default='config/test_config.yaml', type=str,
                      help='prediction config file path')
    
    args = args.parse_args()
    config = read_conf(args.config)
    main(config)
    
    