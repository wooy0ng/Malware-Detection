import logging
import mlflow
import argparse
import numpy as np
import pandas as pd
import model.metric as module_metric
import data_loader.data_loaders as module_data
from pathlib import Path
from utils import read_conf
from logger import setup_logging
from collections import defaultdict
from typing import Optional, Union


def test_logger(mode):
    log_dir = Path('saved/log/predicted')
    log_dir.mkdir(parents=True, exist_ok=True)
    
    setup_logging(save_dir=log_dir)
    logger = logging.getLogger('predicted')
    logger.setLevel(logging.DEBUG)
    
    logger.info("{} mode".format(mode))
    return logger

def main(config) -> Optional[Union[None, int]]:
    # set mlflow tracking directory's path
    mlflow_dir = Path('saved/mlflow')
    tracking_url = 'file://' + str(mlflow_dir.absolute())
    mlflow.set_tracking_uri(tracking_url)
    
    mode = 'evaluation' if config['file'] is None else 'prediction'
    logger = test_logger(mode)   # logging
    
    data_loaders, archs = dict(), dict()
    for dname, arch in zip(config['data_loader']['type'], config['arch']['type']):     # initialize dataloader
        obj = getattr(module_data, dname)
        
        if mode == 'evaluation':
            kwargs = {
                'data_dir': 'data/dataset/unziped',
                'metadata_path': 'data/test_metadata.csv',
                'logger': logger,
                'training': False
            }
        else:
            fpath = Path(config['file']).absolute()
            fparent, fname = fpath.parent, fpath.name
            pd.DataFrame({'file_name': [fname], 'label': [-1]}).to_csv('prediction.csv')
            kwargs = {
                'data_dir': fparent,
                'metadata_path': 'prediction.csv',
                'logger': logger,
                'training': False
            }
        
        if dname.startswith("API") is True:
            kwargs.update({'apidata_dir': "data/apidata"})
        
        data_loaders.update({dname: obj(**kwargs)})
        logger.info(" ")
        logger.info("        {} obj is loaded.".format(dname))
        
        loaded_model = mlflow.pyfunc.load_model(arch)
        archs.update({dname: loaded_model})

    # test
    all_predictions = defaultdict(list)
    all_labels = defaultdict(int)
    metrics = [getattr(module_metric, met) for met in config['metrics']]
    for dname in data_loaders.keys():
        fnames = data_loaders[dname].dataset.embed_passage['file_name']
        dataset = data_loaders[dname].dataset.embed_passage['embed_passage']
        labels = np.array(data_loaders[dname].dataset.embed_passage['label'])
        
        if dname.startswith('Opcode') is not True:
            dataset = pd.DataFrame(dataset)     # sparse to dataframe
            
        loaded_model = archs[dname]
        predicted = loaded_model.predict(dataset)
        
        if mode == 'evaluation':
            for metric_ftn in metrics:
                score = metric_ftn(labels, predicted)
                logger.info(f"{dname} - {metric_ftn.__name__} score : {score}")
            logger.info(" ")
        
        for fname, p, label in zip(fnames, predicted.tolist(), labels.tolist()):
            all_labels[fname] = label
            all_predictions[fname].append(p)
        
    # hard voting
    labels, votes = list(), list()
    for fname in all_labels.keys():
        predictions = pd.Series(all_predictions[fname])
        labels.append(all_labels[fname])
        votes.append(predictions.value_counts().keys()[0])
    
    result = None
    if mode == 'evaluation':
        for metric_ftn in metrics:
            score = metric_ftn(np.array(labels), np.array(votes))
            logger.info(f"hard vote - {metric_ftn.__name__} score : {score}")
    else:
        result = int(votes[0])
    return result


if __name__ == '__main__':
    args = argparse.ArgumentParser(description='Evaluation')
    args.add_argument('-c', '--config', default='config/test_config.yaml', type=str,
                      help='test config file path')
    args.add_argument('-f', '--file', default="0A0CCB6252F7A699228B4B5FFEBFC5042CABC402AA2514723546C90891D83B8F", type=str,
                      help='if you want to make a prediction for a single file, specify the file path (default: None)')
    
    args = args.parse_args()
    
    config = read_conf(args.config)
    config.update({'file': args.file})
    main(config)
    
    